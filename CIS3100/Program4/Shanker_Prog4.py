# Nitya Shanker
# December 6, 2021
# ECE 3100
# Program 4

import pandas as pd
import os
import numpy as np
from scipy.stats import multivariate_normal
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split

# # Convert train.npz to csv file
# data = np.load("train.npz")
# for key, value in data.items():
#     np.savetxt("train" + key + ".csv", value)
#
# # Convert test.npz to csv file
# data = np.load("test.npz")
# for key, value in data.items():
#     np.savetxt("test" + key + ".csv", value)

# Read in data files
FilePath = "/Users/nityashanker/PycharmProjects/Program4/prog4_data.csv"
Data = pd.read_csv(os.path.basename(FilePath))

# trainData = np.load("train.npz", allow_pickle=True)
# testData = np.load("test.npz", allow_pickle=True)


# TASK 1: Generate scatter plot for all data points in prog4_data.csv
dataValues = Data.values

# C0 = dataValues[0:30, 0:2]
# C1 = dataValues[30:60, 0:2]
# C2 = dataValues[60:100, 0:2]

C0 = dataValues[np.where(dataValues[:,2] == 0)]
C0 = C0[:, 0:2]
C1 = dataValues[np.where(dataValues[:,2] == 1)]
C1 = C1[:, 0:2]
C2 = dataValues[np.where(dataValues[:,2] == 2)]
C2 = C2[:, 0:2]

fig1 = plt.figure()
ax1 = fig1.add_subplot(111)
ax1.scatter(C0[:, 0].T, C0[:, 1].T, edgecolors = "face")
ax1.scatter(C1[:, 0].T, C1[:, 1].T, edgecolors = "face")
ax1.scatter(C2[:, 0].T, C2[:, 1].T, edgecolors = "face")

fig1.savefig("S_P of data.png")
fig1.show()


# TASK 2: Estimate the prior probability using the frequency of each class
freq0 = len(C0) / len(dataValues)
freq1 = len(C1) / len(dataValues)
freq2 = len(C2) / len(dataValues)

print("FREQUENCY FOR PROG4_DATA.CSV: ")
print("P(C0) = " + str(freq0) + ", P(C1) = " + str(freq1) + ", P(C2) = " + str(freq2))
print()


# TASK 3: Estimate a likelihood distribution for each class Ci with all data in prog4_data.csv
# using multivariate distribution

# 3.1 Estimate mean vector for each class Ci
mean0 = np.mean(C0, axis = 0)
mean1 = np.mean(C1, axis = 0)
mean2 = np.mean(C2, axis = 0)

# # TEST PRINT
# print("Mean vectors for prog4_data.csv:")
# print(mean0)
# print(mean1)
# print(mean2)
# print()

# 3.2 Estimate covariance matrix for each class Ci
cov0 = np.cov(C0, rowvar = False)
cov1 = np.cov(C1, rowvar = False)
cov2 = np.cov(C2, rowvar = False)

# # TEST PRINT
# print("Covariance matrices for prog4_data.csv:")
# print(cov0)
# print(cov1)
# print(cov2)
# print()

# 3.3 Print mean vector and covariance matrix for each class Ci
print("MEAN OF CATEGORIES FOR PROG4_DATA.CSV:")
print("Mean of category 0: " + str(mean0))
print("Mean of category 1: " + str(mean1))
print("Mean of category 2: " + str(mean2))
print()
print("COVARIANCE OF CATEGORIES FOR PROG4_DATA.CSV:")
print("Covariance of category 0: " + str(cov0))
print("Covariance of category 1: " + str(cov1))
print("Covariance of category 2: " + str(cov2))
print()

# 3.4 Estimate a likelihood distribution for each class Ci using multivariate distribution
rv0 = multivariate_normal(mean0, cov0)
rv1 = multivariate_normal(mean1, cov1)
rv2 = multivariate_normal(mean2, cov2)

# 3.5 Define the Bayesian discriminative function gi for every class i as a Bayesian likelihood estimate,
# obtained by multiplying the likelihood estimate with the prior probability (gi = P(Ci) * P(x|Ci)).
# Make a classification plot containing 3 subplots
# Plot for C0
x0, y0 = np.meshgrid(C0[:, 0].T,C0[:, 1].T)
pos0 = np.dstack((x0, y0))
rv0 = multivariate_normal(mean0,cov0)
ax0 = plt.subplot(111)
ax0.contourf(x0, y0, freq0 * rv0.pdf(pos0))
ax0.scatter(C0[:, 0].T, C0[:, 1].T, facecolors = "b")

# Plot for C1
x1, y1 = np.meshgrid(C1[:, 0].T,C1[:, 1].T)
pos1 = np.dstack((x1, y1))
rv1 = multivariate_normal(mean1,cov1)
ax1 = plt.subplot(111)
ax1.contourf(x1, y1, freq1 * rv1.pdf(pos1))
ax1.scatter(C1[:, 0].T, C1[:, 1].T, facecolors = "b")

# Plot for C2
x2, y2 = np.meshgrid(C2[:, 0].T,C2[:, 1].T)
pos2 = np.dstack((x2, y2))
rv2 = multivariate_normal(mean2,cov2)
ax2 = plt.subplot(111)
ax2.contourf(x2, y2, freq2 * rv2.pdf(pos2))
ax2.scatter(C2[:, 0].T, C2[:, 1].T, facecolors = "b")

plt.xlabel("X1")
plt.ylabel("Y1")
plt.savefig("likelihood function plot for prog4_data.png")
plt.show()

# TASK 4: Generate classification results using the Bayesian likelihood distribution on entire dataset

# 4.1 Apply the results generated by Task 3.4 to the Bayesian likelihood function multiplied by P(Ci)
p0 = freq0 * rv0.pdf(dataValues[:,0:2])
p1 = freq1 * rv1.pdf(dataValues[:,0:2])
p2 = freq2 * rv2.pdf(dataValues[:,0:2])

# print("P0, P1, P2:")
# print(p0)
# print(p1)
# print(p2)
# print()

# 4.2 Write code to classify all data points in data file and print classification results
possibilities = np.c_[p0, p1, p2]
predic = np.argmax(possibilities, axis = 1)

accuracy = sum(1 for x,y in zip(predic, dataValues[:, 2]) if x == y) / len(predic)
print("CLASSIFICATION FOR PROG4_DATA.CSV:")
print("The classification 1 Accuracy on entire set is:", accuracy)
print()

# 4.3 Generate confusion matrix
cm = confusion_matrix(dataValues[:, 2], predic)
cm_display = ConfusionMatrixDisplay(cm).plot()
plt.savefig("Bayesian likelihood classification result on entire set for prog4_data.png")
plt.show()


# TASK 5: Generate classification results with the Bayesian likelihood distributions multiplied by
# prior class probabilities using separated training and test data

# 5.1 Use same code in Task 2 & Task 3 to generate P(Ci), P(X|Ci), and gi using training data in file train.npz
# Task 1 for train.npz
trainData = np.load("train.npz", allow_pickle=True)

# Find key names from train.npz
# for k in trainData.files:
#     print(k)

# Create dataframe
trainValues = np.column_stack((trainData["X_train"], trainData["y_train"]))
# print(trainValues)

TRAIN0 = trainValues[np.where(trainValues[:,2] == 0)]
TRAIN0 = TRAIN0[:, 0:2]
TRAIN1 = trainValues[np.where(trainValues[:,2] == 1)]
TRAIN1 = TRAIN1[:, 0:2]
TRAIN2 = trainValues[np.where(trainValues[:,2] == 2)]
TRAIN2 = TRAIN2[:, 0:2]

# Task 2 for train.npz
trainfreq0 = len(TRAIN0) / len(trainValues)
trainfreq1 = len(TRAIN1) / len(trainValues)
trainfreq2 = len(TRAIN2) / len(trainValues)

print("FREQUENCY FOR TRAIN.NPZ: ")
print("P(TRAIN0) = " + str(trainfreq0) + ", P(TRAIN1) = " + str(trainfreq1) + ", P(TRAIN2) = " + str(trainfreq2))
print()

# Task 3 for train.npz
# TRAIN.NPZ 3.1 Estimate mean vector for each class Ci
trainmean0 = np.mean(TRAIN0, axis = 0)
trainmean1 = np.mean(TRAIN1, axis = 0)
trainmean2 = np.mean(TRAIN2, axis = 0)

# TEST PRINT
# print("Mean vectors for train.npz:")
# print(trainmean0)
# print(trainmean1)
# print(trainmean2)
# print()

# TRAIN.NPZ 3.2 Estimate covariance matrix for each class Ci
traincov0 = np.cov(TRAIN0, rowvar = False)
traincov1 = np.cov(TRAIN1, rowvar = False)
traincov2 = np.cov(TRAIN2, rowvar = False)

# # TEST PRINT
# print("Covariance matrices for train.npz:")
# print(traincov0)
# print(traincov1)
# print(traincov2)
# print()

# TRAIN.NPZ 3.3 Print mean vector and covariance matrix for each class Ci
print("MEAN OF CATEGORIES FOR TRAIN.NPZ:")
print("Mean of category 0: " + str(trainmean0))
print("Mean of category 1: " + str(trainmean1))
print("Mean of category 2: " + str(trainmean2))
print()
print("COVARIANCE OF CATEGORIES FOR TRAIN.NPZ:")
print("Covariance of category 0: " + str(traincov0))
print("Covariance of category 1: " + str(traincov1))
print("Covariance of category 2: " + str(traincov2))
print()

# TRAIN.NPZ 3.4 Estimate a likelihood distribution for each class Ci using multivariate distribution
trainrv0 = multivariate_normal(trainmean0, traincov0)
trainrv1 = multivariate_normal(trainmean1, traincov1)
trainrv2 = multivariate_normal(trainmean2, traincov2)

# TRAIN.NPZ 3.5 Define the Bayesian discriminative function gi for every class i as a Bayesian likelihood estimate,
# obtained by multiplying the likelihood estimate with the prior probability (gi = P(Ci) * P(x|Ci)).
# Make a classification plot containing 3 subplots
# Plot for TRAIN0
x0, y0 = np.meshgrid(TRAIN0[:, 0].T,TRAIN0[:, 1].T)
trainpos0 = np.dstack((x0, y0))
trainrv0 = multivariate_normal(trainmean0,traincov0)
trainax0 = plt.subplot(111)
trainax0.contourf(x0, y0, trainfreq0 * trainrv0.pdf(trainpos0))
trainax0.scatter(TRAIN0[:, 0].T, TRAIN0[:, 1].T, facecolors = "b")

# # Plot for TRAIN1
x1, y1 = np.meshgrid(TRAIN1[:, 0].T,TRAIN1[:, 1].T)
trainpos1 = np.dstack((x1, y1))
trainrv1 = multivariate_normal(trainmean1,traincov1)
trainax1 = plt.subplot(111)
trainax1.contourf(x1, y1, trainfreq1 * trainrv1.pdf(trainpos1))
trainax1.scatter(TRAIN1[:, 0].T, TRAIN1[:, 1].T, facecolors = "b")

# Plot for TRAIN2
x2, y2 = np.meshgrid(TRAIN2[:, 0].T,TRAIN2[:, 1].T)
trainpos2 = np.dstack((x2, y2))
trainrv2 = multivariate_normal(trainmean2,traincov2)
trainax2 = plt.subplot(111)
trainax2.contourf(x2, y2, trainfreq2 * trainrv2.pdf(trainpos2))
trainax2.scatter(TRAIN2[:, 0].T, TRAIN2[:, 1].T, facecolors = "b")

plt.xlabel("X1")
plt.ylabel("Y1")
plt.savefig("likelihood function plot for train data.png")
plt.show()

# 5.2 Generate classification results from test data in test.npz by using same code in Task 4.1 but change
# "dataValues" to "testData"
testData = np.load("test.npz", allow_pickle=True)

# Find key names from test.npz
# for k in testData.files:
#     print(k)

# Create dataframe
testValues = np.column_stack((testData["X_test"], testData["y_test"]))
# print(testValues)

testp0 = trainfreq0 * trainrv0.pdf(testValues[:,0:2])
testp1 = trainfreq1 * trainrv1.pdf(testValues[:,0:2])
testp2 = trainfreq2 * trainrv2.pdf(testValues[:,0:2])

# 5.3 Write code to implement Task 4.2 & 4.3 using data in test.npz instead of data in prog4_data.csv
# TEST.NPZ 4.2 Write code to classify all data points in data file and print classification results
testpossibilities = np.c_[testp0, testp1, testp2]
testpredic = np.argmax(testpossibilities, axis = 1)

testaccuracy = sum(1 for x,y in zip(testpredic, testValues[:, 2]) if x == y) / len(testpredic)
print("CLASSIFICATION FOR TEST.NPZ:")
print("The classification 1 Accuracy on entire set is:", testaccuracy)
print()

# TEST.NPZ 4.3 Generate confusion matrix
testcm = confusion_matrix(testValues[:, 2], testpredic)
testcm_display = ConfusionMatrixDisplay(testcm).plot()
plt.savefig("Bayesian likelihood classification result on entire set for test data.png")
plt.show()


# TASK 6: Extra Credit

# 6.1 For each class Ci (i = 0, 1, 2), find 5 data points in the prog4_data.csv labeled as category i
# that have the top 5 largest gi values
# For C0
p0 = rv0.pdf(C0)
order0 = p0.argsort()[::-1][:5]
top5C0 = C0[order0, :]

# # TEST PRINT
# print("TOP 5 C0: ")
# print(top5C0)

# For C1
p1 = rv1.pdf(C1)
order1 = p1.argsort()[::-1][:5]
top5C1 = C1[order1, :]

# # TEST PRINT
# print("TOP 5 C1: ")
# print(top5C1)

# For C2
p2 = rv2.pdf(C2)
order2 = p2.argsort()[::-1][:5]
top5C2 = C2[order2, :]

# # TEST PRINT
# print("TOP 5 C2: ")
# print(top5C2)

# 6.2 Print the (x1, x2) values for each of these points
print("----- largest 5 points in class 0 -----")
print(top5C0)
print("----- largest 5 points in class 1 -----")
print(top5C1)
print("----- largest 5 points in class 2 -----")
print(top5C2)

# 6.3 Generate a new plot superimposed to the plot generated in Task 3
# Plot for C0
x0, y0 = np.meshgrid(C0[:, 0].T,C0[:, 1].T)
pos0 = np.dstack((x0, y0))
rv0 = multivariate_normal(mean0,cov0)
ax0 = plt.subplot(111)
ax0.contourf(x0, y0, freq0 * rv0.pdf(pos0))
ax0.scatter(C0[:, 0].T, C0[:, 1].T, facecolors = "b")

# Plot for C1
x1, y1 = np.meshgrid(C1[:, 0].T,C1[:, 1].T)
pos1 = np.dstack((x1, y1))
rv1 = multivariate_normal(mean1,cov1)
ax1 = plt.subplot(111)
ax1.contourf(x1, y1, freq1 * rv1.pdf(pos1))
ax1.scatter(C1[:, 0].T, C1[:, 1].T, facecolors = "b")

# Plot for C2
x2, y2 = np.meshgrid(C2[:, 0].T,C2[:, 1].T)
pos2 = np.dstack((x2, y2))
rv2 = multivariate_normal(mean2,cov2)
ax2 = plt.subplot(111)
ax2.contourf(x2, y2, freq2 * rv2.pdf(pos2))
ax2.scatter(C2[:, 0].T, C2[:, 1].T, facecolors = "b")

# Plot for Top 5 Ci
ax1.scatter(top5C0[:, 0].T, top5C0[:, 1].T, edgecolors = "face")
ax1.scatter(top5C1[:, 0].T, top5C1[:, 1].T, edgecolors = "face")
ax1.scatter(top5C2[:, 0].T, top5C2[:, 1].T, edgecolors = "face")

plt.xlabel("X1")
plt.ylabel("Y1")
plt.savefig("likelihood function plot with top 5 points.png")
plt.show()